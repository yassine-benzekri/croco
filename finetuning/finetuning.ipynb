{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T21:46:23.351744Z",
     "iopub.status.busy": "2024-12-18T21:46:23.351515Z",
     "iopub.status.idle": "2024-12-18T21:48:22.629176Z",
     "shell.execute_reply": "2024-12-18T21:48:22.628063Z",
     "shell.execute_reply.started": "2024-12-18T21:46:23.351721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T21:48:22.631057Z",
     "iopub.status.busy": "2024-12-18T21:48:22.630663Z",
     "iopub.status.idle": "2024-12-18T21:48:30.904260Z",
     "shell.execute_reply": "2024-12-18T21:48:30.903162Z",
     "shell.execute_reply.started": "2024-12-18T21:48:22.631012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.1.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "    \n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T21:48:30.905977Z",
     "iopub.status.busy": "2024-12-18T21:48:30.905648Z",
     "iopub.status.idle": "2024-12-18T21:48:36.960838Z",
     "shell.execute_reply": "2024-12-18T21:48:36.960023Z",
     "shell.execute_reply.started": "2024-12-18T21:48:30.905947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T21:48:36.962533Z",
     "iopub.status.busy": "2024-12-18T21:48:36.962128Z",
     "iopub.status.idle": "2024-12-18T21:48:37.716913Z",
     "shell.execute_reply": "2024-12-18T21:48:37.716013Z",
     "shell.execute_reply.started": "2024-12-18T21:48:36.962483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you provide a sentence that illustrates bipolar disorder?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This seroquel withdrawal sucks Problems sleeping even with trazodone, nausea anytime I try to eat or drink even the littlest things, no appetite, depression is coming back with a vengeance. I emailed my pdoc, hopefully I hear back tomorrow. I need something for the nausea at the very least. I can't get my latuda calories down without wanting to throw up. I haven't had a full meal in days. I feel awful. Please don't let this last much longer or please let me go back on seroquel. I'll deal with the weight gain if it means I feel ok. Please please let this all stop and let me feel like a person again.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "# Formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for example in examples['formatted']:  # Assuming your data is in a 'formatted' column\n",
    "        # Extract instruction and response using regex\n",
    "        match = re.search(r\"<s>\\[INST\\](.*?)\\[/INST\\](.*?)</s>\", example, re.DOTALL)\n",
    "        if match:\n",
    "            instruction = match.group(1).strip()\n",
    "            response = match.group(2).strip()\n",
    "            # Format into Llama-3.1 chat template\n",
    "            conversation = [\n",
    "                {\"role\": \"user\", \"content\": instruction},\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            ]\n",
    "            formatted_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(formatted_text)\n",
    "        else:\n",
    "            print(\"Warning: Could not parse example:\", example)\n",
    "    return {\"text\": texts}  # Only return the 'text' key\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"cypsiSAS/transformed_bipolar\", split=\"train\")\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Optionally, check the resulting dataset format:\n",
    "print(dataset[0][\"text\"])  # Check the first element to ensure the formatting is correct\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T22:19:13.863244Z",
     "iopub.status.busy": "2024-12-18T22:19:13.862882Z",
     "iopub.status.idle": "2024-12-18T22:19:14.318532Z",
     "shell.execute_reply": "2024-12-18T22:19:14.317838Z",
     "shell.execute_reply.started": "2024-12-18T22:19:13.863214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 20,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 40,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T22:19:14.320559Z",
     "iopub.status.busy": "2024-12-18T22:19:14.320266Z",
     "iopub.status.idle": "2024-12-18T22:19:14.337035Z",
     "shell.execute_reply": "2024-12-18T22:19:14.336290Z",
     "shell.execute_reply.started": "2024-12-18T22:19:14.320531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T22:19:14.525237Z",
     "iopub.status.busy": "2024-12-18T22:19:14.524907Z",
     "iopub.status.idle": "2024-12-18T22:19:14.549100Z",
     "shell.execute_reply": "2024-12-18T22:19:14.548199Z",
     "shell.execute_reply.started": "2024-12-18T22:19:14.525211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with my emotions to the point of numbness, I get extremely emotional and then numb and then all I want to do is sleep because I\\'ve hyped myself up so much that in the end I feel just nothing like every emotion leaves my body. And then an hour later or even less it\\'s like that again.\\n\\nI have health issues including major depression, Tics and autism. I\\'m not sure if this contributes. I feel like I can never be happy whenever I try I will think of one horrible thing and all my happiness will go down the drain. I\\'m either sad or happy there is no inbetween with me and when I\\'m sad I\\'m extremely low to the point of sale elf destruction. Including binge eating or taking excessive pain killers because it helps with the headaches and my pain from my health issues. \\n\\nI don\\'t necessarily feel like I\\'ve ruined relationships in my life because most people in my life have just been nasty to me including boyfriends and people from school. I\\'ve always been with bad men. People (my mother especially) have said to me \"You\\'re this way because of your BPD and it\\'s not healthy and you need to change\". I\\'m a sensitive person I feel like I\\'m always looking for answers in the wrong place but I\\'m hoping just to get this off my chest.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is a way someone might communicate their despair while under bipolar disorder?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDo you do cocaine to cope with the lows? My PDOC is much more afraid of me having a manic episode with psychosis, than he is about the lows of depression, so he never prescribed me any anti-depressants, even when I was on Olanzapine tired all the time, he would not give me any stimulant which eventually led to me being let go from my last job about a year ago. Today I got some bad news regarding other job opportunity on different continent I was looking forward to... feeling down, I\\'ve reached to my stash and had some white nose powder, how about you and a drug habit? Do you self medicate with illicit substances?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nShare a sentence that indicates a person is coping with the effects of bipolar disorder.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can\\'t trust my memories I had a pretty extended hypomanic episode in Nov-Dec, and a few weeks into the episode I had a long-awaited pdoc appointment at a specialized mood disorder clinic. I came out of the appointment very upset because the doctor didn\\'t seem to want to help me, didn\\'t seem to get what I was saying, wanted me to stop taking the meds that had pulled me out of a year-long suicidal depression. I hadn\\'t slept more than an hour the night before, so the appointment is a bit of a blur, but I felt betrayed by the system and cried for hours.\\n\\nI saw the pdoc for a second time today and my impression of her was completely different. Apparently she is not a psycho monster bitch. She clearly wants to help me and has a reasonable approach, even if it\\'s not what I would have chosen.\\n\\nI also realized that I have gaps in my memory from that time - like I know one of my friends was concerned enough about my state of mind to call my mother - but, for example, I completely don\\'t remember one of my friends coming over and visiting me for several hours. Granted, I was drinking a lot over that period, but not enough to completely forget events. It makes me wonder what else I\\'m not remembering.\\n\\nI\\'m diagnosed BPII but I know that over about 6 weeks I spent way too much money, hardly slept (or sometimes even didn\\'t) for days at a time, and drove so recklessly that I could have been arrested if I had been caught. I just didn\\'t go to work a few times. Also had a few panic attacks. I had some pretty big ideas but I wasn\\'t psychotic, so manic vs. hypomanic? Don\\'t really know, don\\'t really care too much, but I normally consider myself a very grounded and rational person so feeling like I can\\'t trust my memory freaks me the fuck out.\\n\\nI guess this is more of a vent than anything. It\\'s the first hypomanic episode I\\'ve had that can compete with (milder) depressive episodes in terms of negative impact on my life. I don\\'t really know how to come to terms with the fact that I can\\'t trust my memory from that time. Especially because according to friends I seemed pretty lucid, but my judgement was obviously impaired.\\n\\nAnyway, bipolar sucks.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you give an example of a sentence that shows someone is struggling with bipolar disorder?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWoke up from sleep and wrote a suicide note in my groggy state Then I felt relief from my emotions and went back to living my life. Funny isn\\'t it?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat might a person say to convey their state of mind during bipolar disorder?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThink my meds aren’t working. Wanting advice please. Hi. \\nI’ve been on meds to help balance my mood and make me sleep. \\nIt’s the first time I have fully committed to taking meds and it’s definitely made an improvement to how I can handle stress and keeps dark ruminating thoughts to a bearable amount \\nWell at least until these last two months. \\nI’m really struggling and I’m scared to death the meds are becoming ineffective and I don’t want to go back to the way I was.  \\nHas this happened to anyone else? Was it just a glitch for you? \\nI have had a lot of upheaval lately maybe it’s just that? \\nLike I said I’m just a bit scared and wondered if anyone else had got through this?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nShare a sentence that indicates a person\\'s experience with bipolar disorder.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI think I\\'m bipolar, what kind of meds do they prescribe? I never got prescribed meds for bipolar\\nI don\\'t think my therapist figured out I\\'m bipolar,\\nRecently I did some ADHD test and it turned out negative. I then did some MMPI test and results said alot about me having depression and anxiety, (I think I had a manic episode before going into a phase of being depressed and low all the time). I\\'m 19, Idk if im mixing things up but Im gonna be prescribed medication soon and personally I\\'ve tried antidepressants before and hated them, ended up trying to kms with them but it didn\\'t. Wondering what kinds of medications are used for bipolar, and can someone tell me how can I know if I\\'m bipolar or not,(well I mean, I really had around 5 months of a time were I wouldn\\'t sleep unless I\\'m dead tired after at least 2 days of no sleep, I would talk really fast and be super confident, I felt narcissistic alot, I had super creative thoughts all the time and felt so social unlike the times like now where this episode is over) ps. At some point during the episode I took risperdal to sleep (off episode 1 mg would knock me out really good but during the episode I could take 4 mg and still be up, it freaked me out alot till one day I just threw the pills cause I was afraid I might OD) the episode ended on a very bad note with me snapping out on some drunk dude cause we were arguing and beating them so hard I got kicked out of a student hostel I was in. Anywas, ever since I got back to just social withdrawal and spending most of my time alone, my sleep got fixed after like 3 weeks and the first week was a struggle, I could hear voices while I was on bed for hours trying to sleep while all lights were out. Now I guess my depression is caused by my anxiety, I hate antidepressants, what kind of meds do they prescribe. And what\\'s the difference between type 1 and 2<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nShare a sentence that indicates a person\\'s experience with bipolar disorder.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI just need an effing hug, but there are no hugs in my near future. That\\'s pretty much it. I\\'m going through a lot right now, and I\\'m currently very isolated. I\\'m nice, and smart, and funny, and talented, but so fucking lost right now, and I\\'ve been craving physical contact, platonic or romantic doesn\\'t even matter, as long as it\\'s out of real affection. I miss my brothers, my friends, I\\'m so far away from everyone I love and I can\\'t afford a visit. I saw a few posts around these days about how attention starved guys can be, but man, it can get pretty bad for women, too, given the right circumstances.\\n\\nAnyway, I just needed to rant about that. I do fine on most days, but sometimes I crave hugs.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you give an example of a sentence that shows someone is struggling with bipolar disorder?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFp'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T22:19:14.799772Z",
     "iopub.status.busy": "2024-12-18T22:19:14.799384Z",
     "iopub.status.idle": "2024-12-18T22:19:14.824304Z",
     "shell.execute_reply": "2024-12-18T22:19:14.823325Z",
     "shell.execute_reply.started": "2024-12-18T22:19:14.799729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                               \\n\\nDo you do cocaine to cope with the lows? My PDOC is much more afraid of me having a manic episode with psychosis, than he is about the lows of depression, so he never prescribed me any anti-depressants, even when I was on Olanzapine tired all the time, he would not give me any stimulant which eventually led to me being let go from my last job about a year ago. Today I got some bad news regarding other job opportunity on different continent I was looking forward to... feeling down, I've reached to my stash and had some white nose powder, how about you and a drug habit? Do you self medicate with illicit substances?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                        \\n\\nI can't trust my memories I had a pretty extended hypomanic episode in Nov-Dec, and a few weeks into the episode I had a long-awaited pdoc appointment at a specialized mood disorder clinic. I came out of the appointment very upset because the doctor didn't seem to want to help me, didn't seem to get what I was saying, wanted me to stop taking the meds that had pulled me out of a year-long suicidal depression. I hadn't slept more than an hour the night before, so the appointment is a bit of a blur, but I felt betrayed by the system and cried for hours.\\n\\nI saw the pdoc for a second time today and my impression of her was completely different. Apparently she is not a psycho monster bitch. She clearly wants to help me and has a reasonable approach, even if it's not what I would have chosen.\\n\\nI also realized that I have gaps in my memory from that time - like I know one of my friends was concerned enough about my state of mind to call my mother - but, for example, I completely don't remember one of my friends coming over and visiting me for several hours. Granted, I was drinking a lot over that period, but not enough to completely forget events. It makes me wonder what else I'm not remembering.\\n\\nI'm diagnosed BPII but I know that over about 6 weeks I spent way too much money, hardly slept (or sometimes even didn't) for days at a time, and drove so recklessly that I could have been arrested if I had been caught. I just didn't go to work a few times. Also had a few panic attacks. I had some pretty big ideas but I wasn't psychotic, so manic vs. hypomanic? Don't really know, don't really care too much, but I normally consider myself a very grounded and rational person so feeling like I can't trust my memory freaks me the fuck out.\\n\\nI guess this is more of a vent than anything. It's the first hypomanic episode I've had that can compete with (milder) depressive episodes in terms of negative impact on my life. I don't really know how to come to terms with the fact that I can't trust my memory from that time. Especially because according to friends I seemed pretty lucid, but my judgement was obviously impaired.\\n\\nAnyway, bipolar sucks.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                         \\n\\nWoke up from sleep and wrote a suicide note in my groggy state Then I felt relief from my emotions and went back to living my life. Funny isn't it?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                       \\n\\nThink my meds aren’t working. Wanting advice please. Hi. \\nI’ve been on meds to help balance my mood and make me sleep. \\nIt’s the first time I have fully committed to taking meds and it’s definitely made an improvement to how I can handle stress and keeps dark ruminating thoughts to a bearable amount \\nWell at least until these last two months. \\nI’m really struggling and I’m scared to death the meds are becoming ineffective and I don’t want to go back to the way I was.  \\nHas this happened to anyone else? Was it just a glitch for you? \\nI have had a lot of upheaval lately maybe it’s just that? \\nLike I said I’m just a bit scared and wondered if anyone else had got through this?<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                     \\n\\nI think I'm bipolar, what kind of meds do they prescribe? I never got prescribed meds for bipolar\\nI don't think my therapist figured out I'm bipolar,\\nRecently I did some ADHD test and it turned out negative. I then did some MMPI test and results said alot about me having depression and anxiety, (I think I had a manic episode before going into a phase of being depressed and low all the time). I'm 19, Idk if im mixing things up but Im gonna be prescribed medication soon and personally I've tried antidepressants before and hated them, ended up trying to kms with them but it didn't. Wondering what kinds of medications are used for bipolar, and can someone tell me how can I know if I'm bipolar or not,(well I mean, I really had around 5 months of a time were I wouldn't sleep unless I'm dead tired after at least 2 days of no sleep, I would talk really fast and be super confident, I felt narcissistic alot, I had super creative thoughts all the time and felt so social unlike the times like now where this episode is over) ps. At some point during the episode I took risperdal to sleep (off episode 1 mg would knock me out really good but during the episode I could take 4 mg and still be up, it freaked me out alot till one day I just threw the pills cause I was afraid I might OD) the episode ended on a very bad note with me snapping out on some drunk dude cause we were arguing and beating them so hard I got kicked out of a student hostel I was in. Anywas, ever since I got back to just social withdrawal and spending most of my time alone, my sleep got fixed after like 3 weeks and the first week was a struggle, I could hear voices while I was on bed for hours trying to sleep while all lights were out. Now I guess my depression is caused by my anxiety, I hate antidepressants, what kind of meds do they prescribe. And what's the difference between type 1 and 2<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                     \\n\\nI just need an effing hug, but there are no hugs in my near future. That's pretty much it. I'm going through a lot right now, and I'm currently very isolated. I'm nice, and smart, and funny, and talented, but so fucking lost right now, and I've been craving physical contact, platonic or romantic doesn't even matter, as long as it's out of real affection. I miss my brothers, my friends, I'm so far away from everyone I love and I can't afford a visit. I saw a few posts around these days about how attention starved guys can be, but man, it can get pretty bad for women, too, given the right circumstances.\\n\\nAnyway, I just needed to rant about that. I do fine on most days, but sometimes I crave hugs.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                         \\n\\nFp\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T22:19:15.240836Z",
     "iopub.status.busy": "2024-12-18T22:19:15.240449Z",
     "iopub.status.idle": "2024-12-18T22:19:15.246976Z",
     "shell.execute_reply": "2024-12-18T22:19:15.246040Z",
     "shell.execute_reply.started": "2024-12-18T22:19:15.240804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n",
      "6.596 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T01:39:33.951942Z",
     "iopub.status.busy": "2024-12-19T01:39:33.951668Z",
     "iopub.status.idle": "2024-12-19T01:39:35.215192Z",
     "shell.execute_reply": "2024-12-19T01:39:35.214175Z",
     "shell.execute_reply.started": "2024-12-19T01:39:33.951916Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I feel like I am having trouble sleeping and my thoughts are racing all the time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Can you describe your mood over the past week? Have you felt unusually energetic, irritable, or euphoric?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Yes, I felt like I could do anything. I started so many projects but couldn't finish them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Have you experienced any periods where you felt very low, hopeless, or lacked energy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Have you experienced any periods where you felt very low, hopeless, or lacked energy?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Load the bipolar diagnostic chat template specifically for the psychiatrist role\n",
    "chat_template = \"llama-3.1\"\n",
    "\n",
    "# Initialize the tokenizer with the appropriate chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=chat_template,  # LLaMA-based psychiatrist diagnostic template\n",
    ")\n",
    "\n",
    "# Configure the model for faster inference and optimized performance\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Define a set of psychiatric diagnostic prompts focusing on bipolar disorder\n",
    "diagnostic_prompts = [\n",
    "    {\"role\": \"user\", \"content\": \"I feel like I am having trouble sleeping and my thoughts are racing all the time.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Can you describe your mood over the past week? Have you felt unusually energetic, irritable, or euphoric?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Yes, I felt like I could do anything. I started so many projects but couldn't finish them.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Have you experienced any periods where you felt very low, hopeless, or lacked energy?\"},\n",
    "]\n",
    "\n",
    "# Apply the chat template with tokenization for model inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    diagnostic_prompts,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Enable generation for dynamic responses\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate the model's output with specific parameters tuned for clinical interactions\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs, \n",
    "    max_new_tokens=128,  # Longer outputs for detailed psychiatric analysis\n",
    "    use_cache=True,      # Leverage caching for efficiency\n",
    "    temperature=1.2,     # Moderate creativity to adapt to diverse user inputs\n",
    "    min_p=0.15           # Ensure balanced output confidence\n",
    ")\n",
    "\n",
    "# Decode and format the responses for user readability\n",
    "responses = tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Print or log the responses for review by the psychiatrist\n",
    "for response in responses:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T01:42:24.759729Z",
     "iopub.status.busy": "2024-12-19T01:42:24.759353Z",
     "iopub.status.idle": "2024-12-19T01:42:30.980806Z",
     "shell.execute_reply": "2024-12-19T01:42:30.980014Z",
     "shell.execute_reply.started": "2024-12-19T01:42:24.759698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f8b39a47de4d63860d22b8243c6d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a372c727f3d4dbe823d64c651822b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3892ddaa7424d699b6a47b53b72df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Hamatoysin/EMBS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8253989498564e96bb9f90199e1ce8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914334736794665b4677f98c8a3384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "model.push_to_hub(\"Hamatoysin/EMBS\",token = HF_token) # Online saving\n",
    "tokenizer.push_to_hub(\"Hamatoysin/EMBS\", token = HF_token) # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T01:46:24.627321Z",
     "iopub.status.busy": "2024-12-19T01:46:24.626651Z",
     "iopub.status.idle": "2024-12-19T01:46:41.625615Z",
     "shell.execute_reply": "2024-12-19T01:46:41.624822Z",
     "shell.execute_reply.started": "2024-12-19T01:46:24.627290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.1.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Idea overload can be overwhelming and difficult to manage. Here are some strategies that can help you better manage your ideas and channel them in a positive direction:\n",
      "\n",
      "**Categorize your ideas**: \n",
      "\n",
      "Organize your ideas by topic, type (problem-solving, ideas, stories, etc.), or by frequency of thought (e.g., things you've been thinking about daily for weeks). This can help you group related ideas together and find connections that you may have missed otherwise.\n",
      "\n",
      "**Prioritize ideas**: \n",
      "\n",
      "Determine which ideas are the most important, practical, or potentially impactful. Focus on developing and implementing those ideas first.\n",
      "\n",
      "**Free-write or\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I have too many ideas \"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T01:51:04.935663Z",
     "iopub.status.busy": "2024-12-19T01:51:04.935303Z",
     "iopub.status.idle": "2024-12-19T01:54:44.322265Z",
     "shell.execute_reply": "2024-12-19T01:54:44.321227Z",
     "shell.execute_reply.started": "2024-12-19T01:51:04.935633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/ggml-kompute/kompute'\n",
      "Cloning into '/kaggle/working/llama.cpp/ggml/src/ggml-kompute/kompute'...\n",
      "Submodule path 'ggml/src/ggml-kompute/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
      "Requirement already satisfied: gguf in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from gguf) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /opt/conda/lib/python3.10/site-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from gguf) (4.66.4)\n",
      "make: Entering directory '/kaggle/working/llama.cpp'\n",
      "cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by cmake)\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Found CURL: /opt/conda/lib/libcurl.so (found version \"8.10.1\")  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /kaggle/working/llama.cpp/build\n",
      "cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] Built target build_info\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] \u001b[32m\u001b[1mLinking CXX static library libggml-base.a\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] Built target ggml-base\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 42%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX static library libggml-cpu.a\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 47%] Built target ggml-cpu\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 52%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX static library libggml.a\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 57%] Built target ggml\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 76%] Built target llama\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 76%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] Built target common\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] Built target llama-quantize\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target build_info\u001b[0m\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml-base\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[  4%] Built target build_info\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 19%] Built target ggml-base\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml-cpu\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 47%] Built target ggml-cpu\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ggml\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 57%] Built target ggml\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target llama\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 76%] Built target llama\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target common\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] Built target common\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] Built target llama-export-lora\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[  4%] Built target build_info\n",
      "[ 19%] Built target ggml-base\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 47%] Built target ggml-cpu\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 57%] Built target ggml\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 76%] Built target llama\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] Built target common\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n",
      "[100%] Built target llama-cli\n",
      "/usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n",
      "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
      "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
      "Unsloth: Will remove a cached repo with size 2.2G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 20.32 out of 31.35 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 51.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving Hamatoysin/EMBS-G/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving Hamatoysin/EMBS-G/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at Hamatoysin/EMBS-G into q8_0 GGUF format.\n",
      "The output location will be /kaggle/working/Hamatoysin/EMBS-G/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "Writing: 100%|██████████| 3.41G/3.41G [00:57<00:00, 59.6Mbyte/s]\n",
      "Unsloth: Conversion completed! Output location: /kaggle/working/Hamatoysin/EMBS-G/unsloth.Q8_0.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638dd64049f64b5ebea2580ed1c480d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8366e9364445a082326edce93a46a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/Hamatoysin/EMBS-G\n"
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if True: model.push_to_hub_gguf(\"Hamatoysin/EMBS-G\", tokenizer, token = HF_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6124235,
     "sourceId": 9957471,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
